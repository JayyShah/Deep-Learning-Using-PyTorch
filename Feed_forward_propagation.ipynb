{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feed_forward_propagation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk4Fezbb9SZc"
      },
      "source": [
        "### Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T12:46:03.594770Z",
          "start_time": "2020-09-24T12:46:03.589643Z"
        },
        "id": "VytiqjTQgwf4"
      },
      "source": [
        "import numpy as np\n",
        "def feed_forward(inputs, outputs, weights):       \n",
        "    pre_hidden = np.dot(inputs,weights[0])+ weights[1]\n",
        "    hidden = 1/(1+np.exp(-pre_hidden))\n",
        "    pred_out = np.dot(hidden, weights[2]) + weights[3]\n",
        "    mean_squared_error = np.mean(np.square(pred_out - outputs))\n",
        "    return mean_squared_error"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code BreakDown"
      ],
      "metadata": {
        "id": "Da2fkNUH-RzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Takes the values (inputs), weights(randomly initiated if this is the first iteration) and the actual outputs\n",
        "######  as provided in the datasets as the parameters of the `(feed_forward)` function.\n",
        "\n",
        "2. Calculate hidden layer values by performing the matrix multiplication\n",
        "(np.dot) of inputs and weight values (weights[0]) connecting the input\n",
        "layer to the hidden layer and add the bias terms (weights[1]) associated\n",
        "with the hidden layer's nodes.\n",
        "\n",
        "3. Apply the `(sigmoid activation)` function on top of the hidden layer values\n",
        "obtained in the previous step – pre_hidden\n",
        "\n",
        "4. Calculate the output layer values by performing the matrix multiplication\n",
        "(np.dot) of hidden layer activation values (hidden) and weights\n",
        "connecting the hidden layer to the output layer (weights[2]) and\n",
        "summing the output with bias associated with the node in the output layer\n",
        "– weights[3]\n",
        "\n",
        "5. Calculate the `(mean squared error)` value across the dataset and return the\n",
        "mean squared error"
      ],
      "metadata": {
        "id": "TSE9hqwl-ccs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Activation Functions"
      ],
      "metadata": {
        "id": "2hkq4Km8BJX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tanh \n",
        "def tanh(x):\n",
        "  return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "\n",
        "# ReLU - The Rectifier Linear Unit\n",
        "def relu(x):\n",
        "  return np.where(x>0, x,0)\n",
        "\n",
        "# Linear - The linear activation of a value is the value itself\n",
        "def linear(x):\n",
        "  return x\n",
        "\n",
        "# Softmax - Unlike other activations, softmax is performed on top of an\n",
        "# array of values. This is generally done to determine the probability of an\n",
        "# input belonging to one of the m number of possible output classes in a\n",
        "# given scenario. \n",
        "\n",
        "# Softmax activation is used to provide a probability value for each class in\n",
        "# the output\n",
        "\n",
        "def softmax(x):\n",
        "  return np.exp(x)/np.sum(np.exp(x))"
      ],
      "metadata": {
        "id": "bagyHWy5-jXQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two operations on top of input x – np.exp will make all values positive,\n",
        "and the division by np.sum(np.exp(x)) of all such exponents will force all the\n",
        "values to be in between 0 and 1. This range coincides with the probability of an event.\n",
        "And this is what we mean by returning a probability vector. "
      ],
      "metadata": {
        "id": "H39lLjwxDDhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Functions"
      ],
      "metadata": {
        "id": "A2TnKHy8Do2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss values (which are minimized during a neural network training process) are\n",
        "minimized by updating weight values. Defining the proper loss function is the key to\n",
        "building a working and reliable neural network model.\n",
        "\n",
        "\n",
        "**Mean Squared Error** : The mean squared error is the squared difference\n",
        "between the actual and the predicted values of the output.\n",
        "Squaring ensures that positive and negative errors do not offset each other.\n",
        "\n",
        "The mean squared error is typically used when trying to predict a value that is continue in nature.\n",
        "\n",
        "**Mean Absolute Error** :The mean absolute error works in a manner that is\n",
        "very similar to the mean squared error. The mean absolute error ensures\n",
        "that positive and negative errors do not offset each other by taking an\n",
        "average of the absolute difference between the actual and predicted values\n",
        "across all data points.\n",
        "\n",
        "**Binary Cross-Entropy** :Cross-entropy is a measure of the difference between two different distributions: actual and predicted. Binary cross-entropy is applied to binary output data, unlike the previous two loss functions that discussed (which are applied during continuous variable prediction).\n",
        "\n",
        "**Categorical Cross-Entropy** :Categorical cross-entropy between an array of predicted values (p) and an array of actual values (y).\n"
      ],
      "metadata": {
        "id": "r3OIrTNkDxyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MSE - Mean Squared Error\n",
        "def mse(p,y):\n",
        "  return np.mean(np.square(p-y))\n",
        "\n",
        "# 2. MAE - Mean absolute Error\n",
        "def mae(p,y):\n",
        "  return np.mean(np.abs(p-y))\n",
        "\n",
        "# 3. Binary Cross Entropy\n",
        "def binary_cross_entropy(p,y):\n",
        "  return -np.mean(np.sum((y*np.log(p)+(1-y)*np.log(1-p))))\n",
        "\n",
        "# 4 Categorical Cross-Entropy\n",
        "def categorical_cross_entropy(p,y):\n",
        "  return -np.mean(np.sum(y*np.log(p)))"
      ],
      "metadata": {
        "id": "wBEYZDsuDCN5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dfyDwyB-Hr_f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}