{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP43pt/Iuw+TLvkRgSO/Lfh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":13,"metadata":{"id":"c-HtdBTbvoGN","executionInfo":{"status":"ok","timestamp":1672036845848,"user_tz":-330,"elapsed":776,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}}},"outputs":[],"source":["# Define the input (x) and output (y) values:\n","import torch\n","x = [[1,2],[3,4],[5,6],[7,8]]\n","y = [[3],[7],[11],[15]]"]},{"cell_type":"code","source":["# Convert the input lists into tensor objects:\n","\n","X = torch.tensor(x,).float()\n","Y = torch.tensor(y).float()"],"metadata":{"id":"V6Ym2QuHxFgg","executionInfo":{"status":"ok","timestamp":1672036846711,"user_tz":-330,"elapsed":10,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["In the preceding code, we have converted the tensor objects into\n","floating-point objects. It is good practice to have tensor objects as floats or\n","long ints, as they will be multiplied by decimal values (weights) anyway."],"metadata":{"id":"4Gdwzz7hyvTW"}},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","X = X.to(device)\n","Y = Y.to(device)"],"metadata":{"id":"57i-xpYyyqbP","executionInfo":{"status":"ok","timestamp":1672036846711,"user_tz":-330,"elapsed":8,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["## Define the Neural Network Architecture"],"metadata":{"id":"YU3L7lpR9-nb"}},{"cell_type":"code","source":["# The torch.nn module contains functions that help in building neural\n","# network models:\n","\n","import torch.nn as nn\n","\n","class MyNeuralNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.input_to_hidden_layer = nn.Linear(2,8)\n","        self.hidden_layer_activation = nn.ReLU()\n","        self.hidden_to_output_layer = nn.Linear(8,1)\n","    def forward(self, x):\n","        x = self.input_to_hidden_layer(x)\n","        x = self.hidden_layer_activation(x)\n","        x = self.hidden_to_output_layer(x)\n","        return x\n","\n","# print(nn.Linear(2, 7))\n","# Uncomment the above code to check what nn.Linear has as an Output"],"metadata":{"id":"NF7jY8Qwy2WW","executionInfo":{"status":"ok","timestamp":1672036846711,"user_tz":-330,"elapsed":8,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["**Code BreakDown**\n","- Create a class (MyNeuralNet) that can compose our neural\n","network architecture. It is mandatory to inherit from nn.Module\n","when creating a model architecture as it is the base class for all neural\n","network modules.\n","- Within the class,initialize all the components of a neural network\n","using the __init__ method.Call super().__init__() to\n","ensure that the class inherits nn.Module.\n","- Define the layers in the neural network.\n","- Specify all the layers of neural\n","network – a linear layer (self.input_to_hidden_layer), followed\n","by ReLU activation (self.hidden_layer_activation), and finally,\n","a linear layer (self.hidden_to_output_layer).\n","- It is mandatory to use forward as the function name since PyTorch\n","has reserved this function as the method for performing forward\n","propagation. Using any other name in its place will raise an error."],"metadata":{"id":"G8fnJ5qe-W86"}},{"cell_type":"code","source":["# Create an instance of the MyNeuralNet class object that we defined earlier and register it to device:\n","\n","mynet = MyNeuralNet().to(device)\n","\n","# To Obtain parameter of Given  layer\n","# mynet.input_to_hidden_layer.weight\n","\n","# Obtain the Parameters of all the layers in the Neural Network.\n","\n","# mynet.paramters()"],"metadata":{"id":"zCY0LbPm-Rgr","executionInfo":{"status":"ok","timestamp":1672036846712,"user_tz":-330,"elapsed":7,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Define the loss function that we optimize for. Given that we are predicting\n","# for a continuous output, we'll optimize for mean squared error\n","\n","loss_func = nn.MSELoss()"],"metadata":{"id":"Fuw2xFVWAV90","executionInfo":{"status":"ok","timestamp":1672036846712,"user_tz":-330,"elapsed":7,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# The loss value of a neural network can be calculated by passing the input values through the neuralnet object and then\n","# calculating MSELoss for the given inputs:\n","\n","_Y = mynet(X)\n","loss_value = loss_func(_Y,Y)\n","print(loss_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kEpLyG-iAYjL","executionInfo":{"status":"ok","timestamp":1672036846712,"user_tz":-330,"elapsed":6,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}},"outputId":"ad31bb63-1234-44cd-e43e-868885594681"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(92.1403, grad_fn=<MseLossBackward0>)\n"]}]},{"cell_type":"markdown","source":["- In the preceding code, mynet(X) calculates the output values when the\n","input is passed through the neural network. Furthermore, the loss_func\n","function calculates the MSELoss value corresponding to the prediction of\n","the neural network (_Y) and the actual values (Y).\n"],"metadata":{"id":"LEkt5mbbDzAS"}},{"cell_type":"code","source":["# Import the SGD method from the torch.optim module and then pass the neural network object (mynet) and learning rate (lr) as parameters to the\n","# SGD method:\n","\n","from torch.optim import SGD\n","opt = SGD(mynet.parameters(), lr = 0.001)"],"metadata":{"id":"J5sffAO6DqPT","executionInfo":{"status":"ok","timestamp":1672036846713,"user_tz":-330,"elapsed":6,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["- Perform all the steps to be done in an epoch together:\n","Calculate the loss value corresponding to the given input and output.\n","Calculate the gradient corresponding to each parameter.\n","Update the weights based on the learning rate and gradient of each\n","parameter.\n","Once the weights are updated, ensure that the gradients that have\n","been calculated in the previous step are flushed before calculating the\n","gradients in the next epoch."],"metadata":{"id":"DyHFaXAmHufC"}},{"cell_type":"code","source":["loss_history = []\n","for _ in range(50):\n","    opt.zero_grad()\n","    loss_value = loss_func(mynet(X),Y)\n","    loss_value.backward()\n","    opt.step()\n","    loss_history.append(loss_value)"],"metadata":{"id":"SnSq0xYzEKKr","executionInfo":{"status":"ok","timestamp":1672036846713,"user_tz":-330,"elapsed":5,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["- Repeat the preceding steps as many times as the number of epochs\n","using a for loop. In the following example, we are performing the\n","weight update process for a total of 50 epochs. Furthermore, we are\n","storing the loss value in each epoch in the list – loss_history"],"metadata":{"id":"H9yXOhZrH6jp"}},{"cell_type":"code","source":["# import matplotlib.pyplot as plt\n","# %matplotlib inline\n","# plt.plot(loss_history)\n","# plt.title('Loss variation over increasing epochs')\n","# plt.xlabel('epochs')\n","# plt.ylabel('loss value')"],"metadata":{"id":"Pp9_DH3rKTbi","executionInfo":{"status":"ok","timestamp":1672036846713,"user_tz":-330,"elapsed":5,"user":{"displayName":"Jay Shah","userId":"15435979828308902392"}}},"execution_count":22,"outputs":[]}]}